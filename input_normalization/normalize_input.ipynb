{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE-1 IN EACH PROJECT (depending on default values for each column)\n",
    "# Determine default values for each column in case data is missing\n",
    "record_defaults = [[\"\"], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "\n",
    "def decode_csv(line):\n",
    "    parsed_line = tf.decode_csv(line, record_defaults)\n",
    "    label = parsed_line[-1:]          # last column is label\n",
    "    del parsed_line[-1]               # delete the last element from the list   (label column)\n",
    "    del parsed_line[0]                # even delete the first element bcz it is assumed NOT to be a feature\n",
    "    features = tf.stack(parsed_line)  # Stack features so that you can later vectorize forward prop., etc.\n",
    "    label = tf.stack(label)           # Needed bcz labels consist of 2 columns\n",
    "    batch_to_return = features, label\n",
    "\n",
    "    return batch_to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READS DATA FROM train data set CSV FILES, calculates mean (mu) and variance (sigma_square) for all features\n",
    "\n",
    "# ASSUMPTIONS: (Otherwise, decode_csv function needs update)\n",
    "# 1) The first column is NOT a feature. (It is most probably a training example ID or similar)\n",
    "# 2) The last column is always the label. And there is ONLY 1 column that represents the label.\n",
    "#    If more than 1 column represents the label, decode_csv() function needs update \n",
    "# 3) The first row is assumed to include names of the data types (i.e. feature name, label, etc.) so it is skipped\n",
    "\n",
    "def get_input_norm_params(train_input_paths, minibatch_size):\n",
    "\n",
    "    with tf.name_scope(\"next_train_batch\"):\n",
    "        filenames = tf.placeholder(tf.string, shape=[None])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "        dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))\n",
    "        dataset = dataset.batch(minibatch_size)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "    num_examples = 0    # will keep total # train examples\n",
    "    mu = 0              # will keep mean of all feature values\n",
    "    sigma_square = 0    # keeps variance (to be used for scaling)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer, feed_dict={filenames: train_input_paths})\n",
    "        while True:\n",
    "            try:\n",
    "              features, labels = sess.run(next_element)\n",
    "        \n",
    "              num_examples += features.shape[0] #size of axis=0 gives # train examples in the current batch\n",
    "            \n",
    "              # mu = sum_i(features) / num_train_examples  (where i = 1, .., num_train_examples)\n",
    "              mu += tf.reduce_sum(features, axis=[0], keepdims=True)\n",
    "              # sigma_square = sum_i(features ** 2) / num_train_examples  (where i = 1, .., num_train_examples)\n",
    "              sigma_square +=  tf.reduce_sum(tf.multiply(features, features), axis=[0], keepdims=True)\n",
    "            \n",
    "              #print(sess.run(mu))\n",
    "              #print(sess.run(sigma_square))\n",
    "        \n",
    "            except tf.errors.OutOfRangeError:\n",
    "              print(\"Input normalization completed on train set data.\")\n",
    "              break\n",
    "                \n",
    "        mu /= num_examples\n",
    "        sigma_square /= num_examples\n",
    "    \n",
    "        print(\"mu: \\n\", sess.run(mu))\n",
    "        print(\"sigma: \\n\", sess.run(sigma_square))\n",
    "    \n",
    "    return mu, sigma_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READS DATA FROM train data set CSV FILES, normalizes the data and writes normalized values in a new file.\n",
    "# This preprocessing needs to be done first if input normalization is supposed to be applied prior to training\n",
    "# the actual model.\n",
    "\n",
    "# ASSUMPTIONS: (Otherwise, decode_csv function needs update)\n",
    "# 1) The first column is NOT a feature. (It is most probably a training example ID or similar)\n",
    "# 2) The last column is always the label. And there is ONLY 1 column that represents the label.\n",
    "#    If more than 1 column represents the label, decode_csv() function needs update \n",
    "# 3) The first row is assumed to include names of the data types (i.e. feature name, label, etc.) so it is skipped\n",
    "\n",
    "def normalize_train_data(train_input_paths, minibatch_size, mu, sigma_square):\n",
    "\n",
    "    with tf.name_scope(\"read_next_train_batch\"):\n",
    "        filenames = tf.placeholder(tf.string, shape=[None])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "        dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))\n",
    "        dataset = dataset.batch(minibatch_size)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer, feed_dict={filenames: train_input_paths})\n",
    "        while True:\n",
    "            try:\n",
    "              features, labels = sess.run(next_element)\n",
    "              print(\"features:\\n\", features)\n",
    "              #print(sess.run(\"features:\\n\", sess.run(features)))\n",
    "            \n",
    "              # Normalize as ((input_features - mu) / sigma_square)\n",
    "              normalized_features = tf.divide(tf.subtract(features, mu), sigma_square)\n",
    "              print(\"normalized features:\\n\", sess.run(normalized_features))\n",
    "              \n",
    "            except tf.errors.OutOfRangeError:\n",
    "              print(\"All data has been normalized and printed out\")\n",
    "              break\n",
    "                \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input normalization completed on train set data.\n",
      "mu: \n",
      " [[3.409091  3.6818182 1.25      2.4772727 2.8863637]]\n",
      "sigma: \n",
      " [[65.86364  67.90909   7.068182 14.295455 25.704546]]\n",
      "features:\n",
      " [[ 1.  2.  5.  9.  8.]\n",
      " [ 4.  5.  1.  1.  6.]\n",
      " [-4.  5.  1.  0. 12.]\n",
      " [ 1.  4.  7.  4.  2.]\n",
      " [ 2.  2.  4.  8.  2.]\n",
      " [18. 28.  0.  0.  1.]\n",
      " [40.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  9.]\n",
      " [ 4.  7.  3.  1. -8.]\n",
      " [ 2.  3.  1.  1.  4.]]\n",
      "normalized features:\n",
      " [[-0.03657695 -0.02476573  0.5305466   0.45627978  0.19893898]\n",
      " [ 0.0089717   0.01941098 -0.03536977 -0.10333863  0.12113173]\n",
      " [-0.11249137  0.01941098 -0.03536977 -0.17329094  0.35455346]\n",
      " [-0.03657695  0.00468541  0.8135048   0.10651828 -0.03448276]\n",
      " [-0.02139406 -0.02476573  0.3890675   0.38632748 -0.03448276]\n",
      " [ 0.22153208  0.35809907 -0.17684887 -0.17329094 -0.07338639]\n",
      " [ 0.5555555  -0.05421687 -0.17684887 -0.17329094 -0.11229001]\n",
      " [-0.05175983 -0.05421687 -0.17684887 -0.17329094  0.2378426 ]\n",
      " [ 0.0089717   0.04886211  0.24758841 -0.10333863 -0.42351902]\n",
      " [-0.02139406 -0.01004016 -0.03536977 -0.10333863  0.04332449]]\n",
      "features:\n",
      " [[ -5. -10.   4.   4.   7.]\n",
      " [ -4.  -6.   1.   1.   7.]\n",
      " [ -4.  -6.  -2.   2.   8.]\n",
      " [  8.   2.   1.   1.   3.]\n",
      " [  0.   0.   1.   4.   4.]\n",
      " [  2.   4.   0.   6.   1.]\n",
      " [  2.   3.   6.  10.   4.]\n",
      " [  6.   4.   1.  -1.   5.]\n",
      " [  8.   7.  -1.   1.   4.]\n",
      " [  9.   8.   2.   2.  -1.]]\n",
      "normalized features:\n",
      " [[-0.12767425 -0.20147257  0.3890675   0.10651828  0.16003536]\n",
      " [-0.11249137 -0.14257029 -0.03536977 -0.10333863  0.16003536]\n",
      " [-0.11249137 -0.14257029 -0.45980707 -0.03338633  0.19893898]\n",
      " [ 0.06970324 -0.02476573 -0.03536977 -0.10333863  0.00442086]\n",
      " [-0.05175983 -0.05421687 -0.03536977  0.10651828  0.04332449]\n",
      " [-0.02139406  0.00468541 -0.17684887  0.24642289 -0.07338639]\n",
      " [-0.02139406 -0.01004016  0.6720257   0.52623206  0.04332449]\n",
      " [ 0.03933747  0.00468541 -0.03536977 -0.24324323  0.08222811]\n",
      " [ 0.06970324  0.04886211 -0.31832796 -0.10333863  0.04332449]\n",
      " [ 0.08488613  0.06358769  0.10610932 -0.03338633 -0.15119363]]\n",
      "features:\n",
      " [[  2.   3.   1.   8.   1.]\n",
      " [ 11.   9.   2.   1.  -1.]\n",
      " [  0.  30.   1.   2.  -2.]\n",
      " [  1.  12.  -1.   7.  -2.]\n",
      " [  4.   5.  -3.   2.  -6.]\n",
      " [  3.   4.   2.   2.   1.]\n",
      " [ -4.  -9.   5.   3.   8.]\n",
      " [ -5. -10.   2.   0.   6.]\n",
      " [ -8.  -4.  -2.   2.  10.]\n",
      " [  0.   2.  -1.  -2.   3.]]\n",
      "normalized features:\n",
      " [[-0.02139406 -0.01004016 -0.03536977  0.38632748 -0.07338639]\n",
      " [ 0.11525189  0.07831326  0.10610932 -0.10333863 -0.15119363]\n",
      " [-0.05175983  0.3875502  -0.03536977 -0.03338633 -0.19009727]\n",
      " [-0.03657695  0.12248997 -0.31832796  0.31637517 -0.19009727]\n",
      " [ 0.0089717   0.01941098 -0.6012862  -0.03338633 -0.34571177]\n",
      " [-0.00621118  0.00468541  0.10610932 -0.03338633 -0.07338639]\n",
      " [-0.11249137 -0.186747    0.5305466   0.03656597  0.19893898]\n",
      " [-0.12767425 -0.20147257  0.10610932 -0.17329094  0.12113173]\n",
      " [-0.1732229  -0.11311915 -0.45980707 -0.03338633  0.2767462 ]\n",
      " [-0.05175983 -0.02476573 -0.31832796 -0.31319556  0.00442086]]\n",
      "features:\n",
      " [[ 2.  1.  2.  3. -4.]\n",
      " [ 2.  4.  0.  6.  1.]\n",
      " [ 4.  6. -1.  0.  6.]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      " [ 3.  3.  3.  3.  3.]\n",
      " [ 3.  3.  2.  2.  3.]\n",
      " [ 2.  2.  3.  3.  2.]\n",
      " [ 5.  5.  3.  3.  0.]\n",
      " [10. 10.  2.  2. -1.]\n",
      " [10. 10.  2.  2. -2.]]\n",
      "normalized features:\n",
      " [[-0.02139406 -0.0394913   0.10610932  0.03656597 -0.26790452]\n",
      " [-0.02139406  0.00468541 -0.17684887  0.24642289 -0.07338639]\n",
      " [ 0.0089717   0.03413654 -0.31832796 -0.17329094  0.12113173]\n",
      " [-0.02139406 -0.02476573  0.10610932 -0.03338633 -0.03448276]\n",
      " [-0.00621118 -0.01004016  0.24758841  0.03656597  0.00442086]\n",
      " [-0.00621118 -0.01004016  0.10610932 -0.03338633  0.00442086]\n",
      " [-0.02139406 -0.02476573  0.24758841  0.03656597 -0.03448276]\n",
      " [ 0.02415459  0.01941098  0.24758841  0.03656597 -0.11229001]\n",
      " [ 0.10006901  0.09303883  0.10610932 -0.03338633 -0.15119363]\n",
      " [ 0.10006901  0.09303883  0.10610932 -0.03338633 -0.19009727]]\n",
      "features:\n",
      " [[ 7.  3. -4.  1.  8.]\n",
      " [ 3.  7. -4.  6.  3.]\n",
      " [-1.  0.  1.  1.  7.]\n",
      " [ 4.  2.  3. -4.  3.]]\n",
      "normalized features:\n",
      " [[ 0.05452035 -0.01004016 -0.74276525 -0.10333863  0.19893898]\n",
      " [-0.00621118  0.04886211 -0.74276525  0.24642289  0.00442086]\n",
      " [-0.06694271 -0.05421687 -0.03536977 -0.10333863  0.16003536]\n",
      " [ 0.0089717  -0.02476573  0.24758841 -0.45310017  0.00442086]]\n",
      "All data has been normalized and printed out\n"
     ]
    }
   ],
   "source": [
    "# In this micro project, it is chosen to implement input normalization in a way\n",
    "# so that we first calculate mu and sigma_square values based on the entire train\n",
    "# set. The same mu and sigma_square values will be used in training as well as \n",
    "# when validating the model on dev and test sets. With other words, you do not\n",
    "# re-calculate mu and sigma_square for dev and test sets\n",
    "\n",
    "train_input_paths = [\"train1.csv\", \"train2.csv\"]\n",
    "\n",
    "minibatch_size = 10\n",
    "\n",
    "mu, sigma_square = get_input_norm_params(train_input_paths, minibatch_size)\n",
    "\n",
    "normalize_train_data(train_input_paths, minibatch_size, mu, sigma_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function only uses tf.TextLineReader(...) and does not read other types of files i.e. binary files\n",
    "def get_next_mini_batch(batch_size, num_epochs, input_paths, record_defaults):\n",
    "\n",
    "    # string_input_producer creates a FIFO queue for holding the filenames until the reader needs them\n",
    "    filename_queue = tf.train.string_input_producer(input_paths,\n",
    "                                                    num_epochs = num_epochs,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "    # Select the type of reader that will be used to read the CSV files down below.\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "    # reader.read(..) just reads 1 row at a time\n",
    "    key, value = reader.read(filename_queue)\n",
    "\n",
    "    # The decode_csv op parses the result of reader.read_up_to(...) into a list of tensors.\n",
    "    # For instance, col2 below is a list of tensors.\n",
    "    col1, col2, col3, col4, col5, col6, col7, col8 = tf.decode_csv(value, record_defaults)\n",
    "\n",
    "    # define which columns constitute features and which columns are labels and stack them together\n",
    "    features = tf.stack([col2, col3, col4, col5, col6])\n",
    "    labels = tf.stack([col7, col8])\n",
    "\n",
    "\n",
    "    min_after_dequeue = batch_size * 3\n",
    "    capacity = min_after_dequeue + 10 * batch_size\n",
    "\n",
    "    X_mini_batch, Y_mini_batch = tf.train.shuffle_batch([features, labels], \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        capacity=capacity,\n",
    "                                                        min_after_dequeue = min_after_dequeue)\n",
    "    \n",
    "    return X_mini_batch, Y_mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 3. 7. 8. 4.]\n",
      " [6. 6. 6. 6. 6.]\n",
      " [4. 4. 4. 4. 4.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [9. 8. 7. 4. 2.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[ 1.  5.  8.  3.  3.]\n",
      " [11. 11. 11. 11. 11.]\n",
      " [13. 13. 13. 13. 13.]\n",
      " [ 4.  6.  3.  4.  3.]\n",
      " [ 3.  3.  3.  3.  3.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[ 4.  6.  3.  4.  3.]\n",
      " [ 3.  5.  6.  9.  4.]\n",
      " [ 9.  9.  9.  9.  9.]\n",
      " [ 1.  5.  8.  3.  3.]\n",
      " [-1.  2.  3.  8.  5.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[ 9.  8.  7.  4.  2.]\n",
      " [ 3.  2.  8.  9.  1.]\n",
      " [-4.  2.  9.  4.  5.]\n",
      " [ 3.  2.  8.  9.  1.]\n",
      " [ 4.  7.  2.  3.  8.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[ 1.  5.  4.  2.  7.]\n",
      " [ 5.  5.  5.  5.  5.]\n",
      " [ 1.  5.  4.  2.  7.]\n",
      " [-4.  2.  9.  4.  5.]\n",
      " [ 4.  4.  4.  4.  4.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[ 8.  8.  8.  8.  8.]\n",
      " [-1.  2.  3.  8.  5.]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      " [10. 10. 10. 10. 10.]\n",
      " [ 4.  7.  2.  3.  8.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[ 7.  7.  7.  7.  7.]\n",
      " [ 3.  5.  6.  9.  4.]\n",
      " [ 8.  8.  8.  8.  8.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [13. 13. 13. 13. 13.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[ 3.  3.  3.  3.  3.]\n",
      " [ 5.  5.  5.  5.  5.]\n",
      " [ 9.  9.  9.  9.  9.]\n",
      " [ 6.  3.  7.  8.  4.]\n",
      " [11. 11. 11. 11. 11.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[10. 10. 10. 10. 10.]\n",
      " [12. 12. 12. 12. 12.]\n",
      " [12. 12. 12. 12. 12.]\n",
      " [ 7.  7.  7.  7.  7.]\n",
      " [ 6.  6.  6.  6.  6.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Done training, epoch reached\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 5\n",
    "num_epochs = 2\n",
    "# Data may reside in several files. 2 is created here for illustration purposes\n",
    "train_path1 = \"train1.csv\"\n",
    "train_path2 = \"train2.csv\"\n",
    "input_paths = [train_path1, train_path2]\n",
    "# Determine default values for each column in case data is missing\n",
    "record_defaults = [[\"\"], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "X_mini_batch, Y_mini_batch = get_next_mini_batch(batch_size, num_epochs, input_paths, record_defaults)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  init_global_var = tf.global_variables_initializer() \n",
    "  sess.run(init_global_var)\n",
    "  # initializing local variables needed to be able to set num_epochs\n",
    "  init_local_var = tf.local_variables_initializer()   \n",
    "  sess.run(init_local_var)\n",
    "    \n",
    "  # Start populating the filename queue.\n",
    "  # tf.train.start_queue_runners(...) needs to be called before populating the queue ...\n",
    "  # before you call run or eval to execute the read\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "  try:\n",
    "    while not coord.should_stop():\n",
    "      X_mb, Y_mb = sess.run([X_mini_batch, Y_mini_batch])\n",
    "      print(X_mb)\n",
    "      print(Y_mb)\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    print('Done training, epoch reached')\n",
    "  finally:\n",
    "    coord.request_stop()\n",
    "  \n",
    "  coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

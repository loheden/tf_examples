{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "double_seq_generator_for_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2VyXUTVzGHVR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **The generator class below generates batches of data based on following expectations:**\n",
        "\n",
        "*   The aim with the generator is to enable utilization of more than one core during the training of an Machine Learning (ML) model while still guaranteeing that each training example (= each sequence) is only trained once during 1 epoch.\n",
        "*   The generator is supposed to generate sequences from 2 different sources (csv files) for each batch. The idea behind this is that the data being generated is supposed to be fed into a Sequence Model architecture that resembels Siamese Networks. The way it should be thought is that there will be 2 identical RNN networks. One of the generated sequences in each batch will be fed into one of the RNNs and the other sequence will be fed into the second RNN. And at the end of the RNNs, 2 outputs are supposed to be concatenated and processed further depending on the type of the problem being solved by the ML model.\n",
        "*  The generator is supposed to return 3 numpy arrays as (source1_sequence, source2_sequence, label).\n",
        "\n",
        "\n",
        "If still not clear, the code below will hopefully make it clearer.\n"
      ]
    },
    {
      "metadata": {
        "id": "z2zvQAFI6rQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as Keras\n",
        "\n",
        "# Only needed when running the code on Colab\n",
        "from google.colab import files\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bpcJONuB6twX",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "80147164-5abe-4e85-af48-15a1b91de9e1"
      },
      "cell_type": "code",
      "source": [
        "# Upload the files to Colab\n",
        "csv_files = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfd81014-4053-4d1d-aeb3-5a55c9193b4d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bfd81014-4053-4d1d-aeb3-5a55c9193b4d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving five.csv to five.csv\n",
            "Saving four.csv to four.csv\n",
            "Saving one.csv to one.csv\n",
            "Saving six.csv to six.csv\n",
            "Saving three.csv to three.csv\n",
            "Saving two.csv to two.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EKMDQrat678s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0238b836-8069-4215-f9d2-295ae8fc9312"
      },
      "cell_type": "code",
      "source": [
        "# Create a list of files that will be used to generate batches from.\n",
        "file_path_list = []\n",
        "\n",
        "for key, value in csv_files.items():\n",
        "  file_path_list.append(key)\n",
        "  \n",
        "file_path_list"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['five.csv', 'four.csv', 'one.csv', 'six.csv', 'three.csv', 'two.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "u39SAw_r7uPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "deb46e2c-1918-457f-fc86-b811d9cd7b8c"
      },
      "cell_type": "code",
      "source": [
        "# Let's check the content of files. \n",
        "for f in file_path_list:\n",
        "  print(f, \"\\n\", pd.read_csv(f), \"\\n\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "five.csv \n",
            "    col1  col2   col3  col4  col5  col6\n",
            "0     1  five    one   500   500     0\n",
            "1     2  five    two   501   501     0\n",
            "2     3  five  three   502   502     2\n",
            "3     4  five   four   503   503     1\n",
            "4     5  five    six   504   504     0 \n",
            "\n",
            "four.csv \n",
            "    col1  col2   col3  col4  col5  col6\n",
            "0     1  four    one   400   400     2\n",
            "1     2  four    two   401   401     2\n",
            "2     3  four  three   402   402     0\n",
            "3     4  four   five   403   403     1\n",
            "4     5  four    six   404   404     2 \n",
            "\n",
            "one.csv \n",
            "    col1 col2   col3  col4  col5  col6\n",
            "0     1  one    two   100   100     0\n",
            "1     2  one  three   101   101     1\n",
            "2     3  one   four   102   102     2\n",
            "3     4  one   five   103   103     0\n",
            "4     5  one    six   104   104     1 \n",
            "\n",
            "six.csv \n",
            "    col1 col2   col3  col4  col5  col6\n",
            "0     1  six    one   600   600     1\n",
            "1     2  six    two   601   601     1\n",
            "2     3  six  three   602   602     1\n",
            "3     4  six   four   603   603     2\n",
            "4     5  six   five   604   604     0 \n",
            "\n",
            "three.csv \n",
            "    col1   col2  col3  col4  col5  col6\n",
            "0     1  three   one   300   300     1\n",
            "1     2  three   two   301   301     1\n",
            "2     3  three  four   302   302     0\n",
            "3     4  three  five   303   303     2\n",
            "4     5  three   six   304   304     1 \n",
            "\n",
            "two.csv \n",
            "    col1 col2   col3  col4  col5  col6\n",
            "0     1  two    one   200   200     0\n",
            "1     2  two  three   201   201     1\n",
            "2     3  two   four   202   202     2\n",
            "3     4  two   five   203   203     0\n",
            "4     5  two    six   204   204     1 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XzEjEjl3JnrY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point, let's define how our batches are expected to look:\n",
        "\n",
        "Assumptions:\n",
        "- Let's just look at a 1 batch with 1 single train example in it.\n",
        "- In each file, columns 2 and 3 indicate the names of the sources. \n",
        "- Column 6 gives the label.\n",
        "- Each row includes features for each timestep. Columns 2,3,6 need to be removed when preparing batch data.\n",
        "- Assume that sequence_length is 2.\n",
        "- If we start from one.csv, we will need to fetch 2 timesteps (rows) and it will be the following (before removing the columns 2,3,6):\n",
        "    [1, one, two, 100, 100, 0]\n",
        "    \n",
        "    [2, one, three, 101, 101, 1]\n",
        "- Label of the batch will be the last column value from the last timestep, whch is \"1\" in this case.\n",
        "- Above, column 2 is giving the source1 (which is \"one\") here. Column 3 gives the source2, which is \"three\". As you see, there are 2 different source2 values here: \"two\" and \"three\". In this project, the source2 of the last timestep is taken as reference when determining the second source to fetch the data from, which is \"three\" in this exmaple.\n",
        "- And we fetch the same rows (timesteps) from three.csv as well as following:\n",
        "\n",
        "  [1, three, one, 300, 300, 1]\n",
        "\n",
        "  [2, three, two, 301, 301, 1]\n",
        "  \n",
        "- Since we now know which rows/sources will be used for our batch, we now can remove unnecessary columns. It will look like the following:\n",
        "- For the sequence from one.csv:\n",
        "\n",
        "    [1, 100, 100]\n",
        "    \n",
        "    [2, 101, 101]\n",
        " \n",
        " - For the sequence from three.csv:\n",
        " \n",
        "  [1, 300, 300]\n",
        "\n",
        "  [2, 301, 301]\n",
        "  \n",
        "- And that's it. The whole batch will look like the following:\n",
        "\n",
        "      [array([[[  1., 100., 100.],  [  2., 101., 101.]]], \n",
        "       array([[[  1., 300., 300.],  [  2., 301., 301.]]])], \n",
        "       array([[0., 1., 0.])\n",
        "       \n",
        " which essentially is: ( [sequence_from_source1,  sequence_from_source2],  label )      \n",
        " \n",
        " Maybe worth to mention is that the label is in the form of a one-hot array to be able to use in with softmax activation."
      ]
    },
    {
      "metadata": {
        "id": "_Pu6xhDY7NK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataGenerator(Keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, file_path_list, nr_header_rows, nr_columns, sequence_length, \n",
        "                 batch_size, total_nr_timesteps_per_csv_file, nr_classes, columns_to_remove, shuffle):\n",
        "        'Initialization'\n",
        "        self.nr_header_rows = nr_header_rows\n",
        "        self.nr_columns = nr_columns\n",
        "        self.sequence_length = sequence_length\n",
        "        self.nr_features = nr_columns - len(columns_to_remove)\n",
        "        self.total_nr_timesteps_per_csv_file = total_nr_timesteps_per_csv_file\n",
        "        self.total_nr_sequences_per_csv_file = self.total_nr_timesteps_per_csv_file - self.sequence_length + 1\n",
        "        self.total_nr_sequences_in_dataset = len(file_path_list) * self.total_nr_sequences_per_csv_file\n",
        "        # If given batch size is bigger than max nr of examples, set the batch size to max nr examples.\n",
        "        self.batch_size = min(batch_size, self.total_nr_sequences_in_dataset)\n",
        "        self.nr_classes = nr_classes\n",
        "        self.columns_to_remove = columns_to_remove\n",
        "        self.shuffle = shuffle\n",
        "        self.prediction_timesteps = np.arange(start = self.sequence_length, \n",
        "                                          stop  = self.total_nr_timesteps_per_csv_file + 1, \n",
        "                                          step  = 1, \n",
        "                                          dtype = np.int32)\n",
        "\n",
        "        self.file_path_list = file_path_list\n",
        "        self.file_path_dict = {}\n",
        "        for csv_file in file_path_list:\n",
        "          self.file_path_dict[csv_file[:-4]] = csv_file\n",
        "        \n",
        "        self.on_epoch_end()\n",
        "\n",
        "        \n",
        "    # triggered once at the very beginning as well as at the end of each epoch\n",
        "    def on_epoch_end(self):\n",
        "        'Updates after each epoch'\n",
        "        if self.shuffle == True:\n",
        "          np.random.shuffle(self.prediction_timesteps)\n",
        "          np.random.shuffle(self.file_path_list)\n",
        "          \n",
        "    'this function will help Keras to control how many epochs the model has run so far..'\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of batches per epoch'\n",
        "        'Note that we round up the division result with np.ceil(..)'    \n",
        "        return int(np.ceil(self.total_nr_sequences_in_dataset / self.batch_size))\n",
        "\n",
        "    'Returns a complete batch'\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        \n",
        "        # which file to start reading data from. int(..) rounds down the result of division\n",
        "        # '% ...' added at the end in case the division becomes equal to 'len(self.file_path_list)'\n",
        "        file_index = int((index * self.batch_size) / self.total_nr_sequences_per_csv_file) % len(self.file_path_list)\n",
        "        # which sample to start reading data from in the file\n",
        "        prediction_timestep_index = (index * self.batch_size) % self.total_nr_sequences_per_csv_file\n",
        "        \n",
        "        (X, y) = self.__data_generation(file_index, prediction_timestep_index)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    'Actual function that generates data fom csv files'\n",
        "    def __data_generation(self, file_index, prediction_timestep_index):\n",
        "      \n",
        "      source1_batch_data_array = np.empty((0, self.sequence_length, self.nr_features))\n",
        "      source2_batch_data_array = np.empty((0, self.sequence_length, self.nr_features))\n",
        "      label_array = []      \n",
        "      \n",
        "      for batch_idx in range(self.batch_size):\n",
        "        print('batch_idx   ', batch_idx)\n",
        "        df_source1 = pd.read_csv(self.file_path_list[file_index], \n",
        "                              header    = None, \n",
        "                              skiprows  = self.nr_header_rows + self.prediction_timesteps[prediction_timestep_index] - self.sequence_length, \n",
        "                              nrows     = self.sequence_length)\n",
        "        #label from the last timestep of the sequence\n",
        "        label = df_source1.values[self.sequence_length-1, self.nr_columns-1] \n",
        "        label_array.append(label)\n",
        "        source2 = df_source1.values[self.sequence_length-1, 2]\n",
        "        source1 = df_source1.values[self.sequence_length-1, 1]\n",
        "        df_source1 = df_source1.drop(columns=self.columns_to_remove, axis=1)\n",
        "        print(\"source1: \", source1)\n",
        "        print(\"source2: \", source2)\n",
        "        print(\"label: \", label)\n",
        "        # first dim = 1 bcz we generate 1 sequence at a time and append it to the batch array\n",
        "        source1_batch_data = np.reshape(df_source1.values, (1, self.sequence_length, self.nr_features))\n",
        "        source1_batch_data_array = np.append(source1_batch_data_array, source1_batch_data, axis=0)\n",
        "\n",
        "        df_source2 = pd.read_csv(self.file_path_dict[source2],\n",
        "                                  header    = None, \n",
        "                                  skiprows  = self.nr_header_rows + self.prediction_timesteps[prediction_timestep_index] - self.sequence_length, \n",
        "                                  nrows     = self.sequence_length)\n",
        "        df_source2 = df_source2.drop(columns=self.columns_to_remove, axis=1)\n",
        "        source2_batch_data = np.reshape(df_source2.values, (1, self.sequence_length, self.nr_features))\n",
        "        source2_batch_data_array = np.append(source2_batch_data_array, source2_batch_data, axis=0)\n",
        "        \n",
        "        prediction_timestep_index = (prediction_timestep_index + 1) % self.total_nr_sequences_per_csv_file\n",
        "        if prediction_timestep_index == 0:\n",
        "          file_index = (file_index + 1) % len(self.file_path_list)\n",
        "      \n",
        "      #print(source1_batch_data_array.shape, source2_batch_data_array.shape, len(label_array))\n",
        "      return ([source1_batch_data_array, source2_batch_data_array], Keras.utils.to_categorical(label_array, num_classes=self.nr_classes))    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DlBqQQK0Ch2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initialization_params = {'file_path_list'                   : file_path_list,  # list of files to fetch data from\n",
        "                         'nr_header_rows'                   : 1,        # nr of rows to ignore from the top in each csv file \n",
        "                         'nr_columns'                       : 6,        # total nr of columns in each csv file \n",
        "                         'sequence_length'                  : 2,        # sequence length of each (train) sample\n",
        "                         'batch_size'                       : 2,        # nr examples to generate in each step\n",
        "                         'total_nr_timesteps_per_csv_file'  : 5,        # total nr of weeks per season\n",
        "                         'nr_classes'                       : 3,        # nr of classes in output layer\n",
        "                         'columns_to_remove'                : [1,2,5],  # column indexes that are not part of the feature list\n",
        "                         'shuffle'                          : False}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AN1GwadbDKBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3768
        },
        "outputId": "c5c0cfb3-9eda-410f-d37d-2eecfea963dd"
      },
      "cell_type": "code",
      "source": [
        "generator = DataGenerator(**initialization_params)\n",
        "for index in range (generator.__len__()):\n",
        "  print(generator.__getitem__(index))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_idx    0\n",
            "source1:  five\n",
            "source2:  two\n",
            "label:  0\n",
            "batch_idx    1\n",
            "source1:  five\n",
            "source2:  three\n",
            "label:  2\n",
            "([array([[[  1., 500., 500.],\n",
            "        [  2., 501., 501.]],\n",
            "\n",
            "       [[  2., 501., 501.],\n",
            "        [  3., 502., 502.]]]), array([[[  1., 200., 200.],\n",
            "        [  2., 201., 201.]],\n",
            "\n",
            "       [[  2., 301., 301.],\n",
            "        [  3., 302., 302.]]])], array([[1., 0., 0.],\n",
            "       [0., 0., 1.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  five\n",
            "source2:  four\n",
            "label:  1\n",
            "batch_idx    1\n",
            "source1:  five\n",
            "source2:  six\n",
            "label:  0\n",
            "([array([[[  3., 502., 502.],\n",
            "        [  4., 503., 503.]],\n",
            "\n",
            "       [[  4., 503., 503.],\n",
            "        [  5., 504., 504.]]]), array([[[  3., 402., 402.],\n",
            "        [  4., 403., 403.]],\n",
            "\n",
            "       [[  4., 603., 603.],\n",
            "        [  5., 604., 604.]]])], array([[0., 1., 0.],\n",
            "       [1., 0., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  four\n",
            "source2:  two\n",
            "label:  2\n",
            "batch_idx    1\n",
            "source1:  four\n",
            "source2:  three\n",
            "label:  0\n",
            "([array([[[  1., 400., 400.],\n",
            "        [  2., 401., 401.]],\n",
            "\n",
            "       [[  2., 401., 401.],\n",
            "        [  3., 402., 402.]]]), array([[[  1., 200., 200.],\n",
            "        [  2., 201., 201.]],\n",
            "\n",
            "       [[  2., 301., 301.],\n",
            "        [  3., 302., 302.]]])], array([[0., 0., 1.],\n",
            "       [1., 0., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  four\n",
            "source2:  five\n",
            "label:  1\n",
            "batch_idx    1\n",
            "source1:  four\n",
            "source2:  six\n",
            "label:  2\n",
            "([array([[[  3., 402., 402.],\n",
            "        [  4., 403., 403.]],\n",
            "\n",
            "       [[  4., 403., 403.],\n",
            "        [  5., 404., 404.]]]), array([[[  3., 502., 502.],\n",
            "        [  4., 503., 503.]],\n",
            "\n",
            "       [[  4., 603., 603.],\n",
            "        [  5., 604., 604.]]])], array([[0., 1., 0.],\n",
            "       [0., 0., 1.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  one\n",
            "source2:  three\n",
            "label:  1\n",
            "batch_idx    1\n",
            "source1:  one\n",
            "source2:  four\n",
            "label:  2\n",
            "([array([[[  1., 100., 100.],\n",
            "        [  2., 101., 101.]],\n",
            "\n",
            "       [[  2., 101., 101.],\n",
            "        [  3., 102., 102.]]]), array([[[  1., 300., 300.],\n",
            "        [  2., 301., 301.]],\n",
            "\n",
            "       [[  2., 401., 401.],\n",
            "        [  3., 402., 402.]]])], array([[0., 1., 0.],\n",
            "       [0., 0., 1.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  one\n",
            "source2:  five\n",
            "label:  0\n",
            "batch_idx    1\n",
            "source1:  one\n",
            "source2:  six\n",
            "label:  1\n",
            "([array([[[  3., 102., 102.],\n",
            "        [  4., 103., 103.]],\n",
            "\n",
            "       [[  4., 103., 103.],\n",
            "        [  5., 104., 104.]]]), array([[[  3., 502., 502.],\n",
            "        [  4., 503., 503.]],\n",
            "\n",
            "       [[  4., 603., 603.],\n",
            "        [  5., 604., 604.]]])], array([[1., 0., 0.],\n",
            "       [0., 1., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  six\n",
            "source2:  two\n",
            "label:  1\n",
            "batch_idx    1\n",
            "source1:  six\n",
            "source2:  three\n",
            "label:  1\n",
            "([array([[[  1., 600., 600.],\n",
            "        [  2., 601., 601.]],\n",
            "\n",
            "       [[  2., 601., 601.],\n",
            "        [  3., 602., 602.]]]), array([[[  1., 200., 200.],\n",
            "        [  2., 201., 201.]],\n",
            "\n",
            "       [[  2., 301., 301.],\n",
            "        [  3., 302., 302.]]])], array([[0., 1., 0.],\n",
            "       [0., 1., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  six\n",
            "source2:  four\n",
            "label:  2\n",
            "batch_idx    1\n",
            "source1:  six\n",
            "source2:  five\n",
            "label:  0\n",
            "([array([[[  3., 602., 602.],\n",
            "        [  4., 603., 603.]],\n",
            "\n",
            "       [[  4., 603., 603.],\n",
            "        [  5., 604., 604.]]]), array([[[  3., 402., 402.],\n",
            "        [  4., 403., 403.]],\n",
            "\n",
            "       [[  4., 503., 503.],\n",
            "        [  5., 504., 504.]]])], array([[0., 0., 1.],\n",
            "       [1., 0., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  three\n",
            "source2:  two\n",
            "label:  1\n",
            "batch_idx    1\n",
            "source1:  three\n",
            "source2:  four\n",
            "label:  0\n",
            "([array([[[  1., 300., 300.],\n",
            "        [  2., 301., 301.]],\n",
            "\n",
            "       [[  2., 301., 301.],\n",
            "        [  3., 302., 302.]]]), array([[[  1., 200., 200.],\n",
            "        [  2., 201., 201.]],\n",
            "\n",
            "       [[  2., 401., 401.],\n",
            "        [  3., 402., 402.]]])], array([[0., 1., 0.],\n",
            "       [1., 0., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  three\n",
            "source2:  five\n",
            "label:  2\n",
            "batch_idx    1\n",
            "source1:  three\n",
            "source2:  six\n",
            "label:  1\n",
            "([array([[[  3., 302., 302.],\n",
            "        [  4., 303., 303.]],\n",
            "\n",
            "       [[  4., 303., 303.],\n",
            "        [  5., 304., 304.]]]), array([[[  3., 502., 502.],\n",
            "        [  4., 503., 503.]],\n",
            "\n",
            "       [[  4., 603., 603.],\n",
            "        [  5., 604., 604.]]])], array([[0., 0., 1.],\n",
            "       [0., 1., 0.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  two\n",
            "source2:  three\n",
            "label:  1\n",
            "batch_idx    1\n",
            "source1:  two\n",
            "source2:  four\n",
            "label:  2\n",
            "([array([[[  1., 200., 200.],\n",
            "        [  2., 201., 201.]],\n",
            "\n",
            "       [[  2., 201., 201.],\n",
            "        [  3., 202., 202.]]]), array([[[  1., 300., 300.],\n",
            "        [  2., 301., 301.]],\n",
            "\n",
            "       [[  2., 401., 401.],\n",
            "        [  3., 402., 402.]]])], array([[0., 1., 0.],\n",
            "       [0., 0., 1.]], dtype=float32))\n",
            "batch_idx    0\n",
            "source1:  two\n",
            "source2:  five\n",
            "label:  0\n",
            "batch_idx    1\n",
            "source1:  two\n",
            "source2:  six\n",
            "label:  1\n",
            "([array([[[  3., 202., 202.],\n",
            "        [  4., 203., 203.]],\n",
            "\n",
            "       [[  4., 203., 203.],\n",
            "        [  5., 204., 204.]]]), array([[[  3., 502., 502.],\n",
            "        [  4., 503., 503.]],\n",
            "\n",
            "       [[  4., 603., 603.],\n",
            "        [  5., 604., 604.]]])], array([[1., 0., 0.],\n",
            "       [0., 1., 0.]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VRb8UQ5BDo-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows how to use tf.metrics.accuracy(..), tf.metrics.precision(..), and \n",
    "# tf.metrics.recall functions. And at the end,F1 score is calculated for\n",
    "# both train and test sets.\n",
    "\n",
    "# IMPORTANT NOTES:\n",
    "# 1) Since the same tensor is used for accuracy/precision/recall calculation for\n",
    "#    both train and test sets, you need to reset local variables in each tf.metrics\n",
    "#    tensor before using it for - say - test set after using it for train set.\n",
    "#    Otherwise, your - say - test results will be a combination of train and test results\n",
    "# 2) You could have used the default tf local variable initializer. But it initializes all\n",
    "#    local variables. It does not heart to use it in the beginning but before re-using \n",
    "#    the tf.metric function, you just need to reset the corresponding local variables\n",
    "# 3) Note that tf.metrics.accuracy(..) function works fine with multi class scenarios.\n",
    "#    However, precision and recall functions are not working when there are more than 2 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUMPTIONS: (Otherwise, decode_csv function needs update)\n",
    "# 1) The first column is NOT a feature. (It is most probably a training example ID or similar)\n",
    "# 2) The last column is always the label. And there is ONLY 1 column that represents the label.\n",
    "#    If more than 1 column represents the label, decode_csv() function needs update \n",
    "# 3) The first row is assumed to include names of the data types (i.e. feature name, label, etc.) \n",
    "#    so it is skipped\n",
    "\n",
    "# UPDATE record_default IN EACH PROJECT (depending on default values for each column)\n",
    "# Determine default values for each column in case data is missing\n",
    "record_defaults = [[\"\"], [0.0], [0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "def decode_csv(line):\n",
    "    parsed_line = tf.decode_csv(line, record_defaults)\n",
    "    label = parsed_line[-1:]          # last column is label\n",
    "    del parsed_line[-1]               # delete the last element from the list   (label column)\n",
    "    del parsed_line[0]                # even delete the first element bcz it is assumed NOT to be a feature\n",
    "    features = tf.stack(parsed_line)  # Stack features so that you can later vectorize forward prop., etc.\n",
    "    label = tf.stack(label)           # Needed bcz labels consist of 2 columns\n",
    "    batch_to_return = features, label\n",
    "\n",
    "    return batch_to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: For simplicity, train.csv is used as both train and test set data.\n",
    "\n",
    "def validation(train_input_paths, minibatch_size, num_classes):\n",
    "    \n",
    "    with tf.name_scope(\"read_next_train_batch\"):\n",
    "        filenames = tf.placeholder(tf.string, shape=[None])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "        dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))\n",
    "        dataset = dataset.batch(minibatch_size)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "    # Placeholders to take in batches of data\n",
    "    tf_labels = tf.placeholder(dtype=tf.int64, shape=[minibatch_size, 1])\n",
    "    tf_predictions = tf.placeholder(dtype=tf.int64, shape=[minibatch_size, 1])        \n",
    "\n",
    "    # IMPORTANT: Never run both return tensors below at the same time as sess.run([accuracy, accuracy_update])\n",
    "    # ALWAYS run them separately as shown below\n",
    "    with tf.name_scope(\"metric_accuracy\"):\n",
    "        # Define the metric and update operations\n",
    "        accuracy, accuracy_update = tf.metrics.accuracy(tf_labels,\n",
    "                                                        tf_predictions)\n",
    "\n",
    "    with tf.name_scope(\"metric_precision\"):\n",
    "        # Define the metric and update operations\n",
    "        #precision, precision_update = tf.metrics.precision(tf_labels,\n",
    "        #                                                   tf_predictions)\n",
    "        precision, precision_update = tf.metrics.precision(tf_labels,\n",
    "                                                           tf_predictions)\n",
    "\n",
    "    with tf.name_scope(\"metric_recall\"):\n",
    "        # Define the metric and update operations\n",
    "        recall, recall_update = tf.metrics.recall(tf_labels,\n",
    "                                                  tf_predictions)        \n",
    "   \n",
    "    with tf.name_scope(\"reset_metric_variables\"):\n",
    "        # Isolate the variables stored behind the scenes by the metric operation\n",
    "        accuracy_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metric_accuracy\")\n",
    "        precision_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metric_precision\")\n",
    "        recall_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metric_recall\")\n",
    "        metrics_var_list = accuracy_vars + precision_vars + recall_vars\n",
    "        # Define initializer to initialize/reset running variables\n",
    "        running_vars_initializer = tf.variables_initializer(var_list=metrics_var_list)        \n",
    "        \n",
    "    init_local_var = tf.local_variables_initializer()   \n",
    "    \n",
    "    # Assume that following values are our predictions from our model\n",
    "    train_predictions = [[[0], [0], [0], [1], [1], [1]],\n",
    "                         [[1], [1], [1], [1], [1], [1]],\n",
    "                         [[1], [1], [1], [1], [1], [1]]]\n",
    "\n",
    "    # Assume that following values are our predictions from our model\n",
    "    test_predictions = [[[0], [0], [0], [0], [0], [0]],\n",
    "                        [[1], [1], [1], [1], [1], [1]],\n",
    "                        [[1], [1], [1], [1], [1], [1]]]    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_local_var)\n",
    "        sess.run(iterator.initializer, feed_dict={filenames: train_input_paths})\n",
    "        \n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "              features, labels = sess.run(next_element)\n",
    "              print(\"labels:\\n\", labels)\n",
    "              print(\"labels.shape: \", labels.shape, \"\\n\")\n",
    "            \n",
    "              print(\"train_predictions[%d]: \" % i, train_predictions[i], \"\\n\")\n",
    "            \n",
    "              # Update the accuracy for the current batch. The function will anyway\n",
    "              # update it based on previous batches as well.\n",
    "              sess.run([accuracy_update, precision_update, recall_update], \n",
    "                       feed_dict={tf_labels: labels, tf_predictions: train_predictions[i]})\n",
    "              i += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "              print(\"All data processed.\\n\")\n",
    "              break\n",
    "        \n",
    "        # Now get the accuracy, precision, and recall values\n",
    "        acc, pre, rec = sess.run([accuracy, precision, recall])\n",
    "        print(\"accuracy: \", acc, \"   precision: \", pre, \"   recall: \", rec)\n",
    "        \n",
    "        f1_score = (2 * pre * rec) / (pre + rec)\n",
    "        print(\"Train - F1 Score: \", f1_score, \"\\n\\n#######\\n\")\n",
    "            \n",
    "        # Before calculating accuracy on test set, reset local variables in your metric_accuracy.\n",
    "        # If not done, the test accuracy result will be the combined result of train + test accuracy results\n",
    "        sess.run(running_vars_initializer)    \n",
    "        \n",
    "        sess.run(iterator.initializer, feed_dict={filenames: train_input_paths})\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "              features, labels = sess.run(next_element)\n",
    "              print(\"labels:\\n\", labels)\n",
    "              print(\"labels.shape: \", labels.shape, \"\\n\")\n",
    "            \n",
    "              print(\"test_predictions[%d]: \" % i, test_predictions[i], \"\\n\")\n",
    "            \n",
    "              # Update the accuracy based on the existing batch\n",
    "              sess.run([accuracy_update, precision_update, recall_update], \n",
    "                       feed_dict={tf_labels: labels, tf_predictions: test_predictions[i]})\n",
    "              i += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "              print(\"All data processed.\\n\")\n",
    "              break\n",
    "\n",
    "        # Now get the accuracy, precision, and recall values\n",
    "        acc, pre, rec = sess.run([accuracy, precision, recall])\n",
    "        print(\"accuracy: \", acc, \"   precision: \", pre, \"   recall: \", rec)  \n",
    "        \n",
    "        f1_score = (2 * pre * rec) / (pre + rec)\n",
    "        print(\"Test - F1 Score: \", f1_score, \"\\n\\n#######\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "labels.shape:  (6, 1) \n",
      "\n",
      "train_predictions[0]:  [[0], [0], [0], [1], [1], [1]] \n",
      "\n",
      "labels:\n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "labels.shape:  (6, 1) \n",
      "\n",
      "train_predictions[1]:  [[1], [1], [1], [1], [1], [1]] \n",
      "\n",
      "labels:\n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "labels.shape:  (6, 1) \n",
      "\n",
      "train_predictions[2]:  [[1], [1], [1], [1], [1], [1]] \n",
      "\n",
      "All data processed.\n",
      "\n",
      "accuracy:  0.8333333    precision:  0.8    recall:  1.0\n",
      "Train - F1 Score:  0.8888889256818805 \n",
      "\n",
      "#######\n",
      "\n",
      "labels:\n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "labels.shape:  (6, 1) \n",
      "\n",
      "test_predictions[0]:  [[0], [0], [0], [0], [0], [0]] \n",
      "\n",
      "labels:\n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "labels.shape:  (6, 1) \n",
      "\n",
      "test_predictions[1]:  [[1], [1], [1], [1], [1], [1]] \n",
      "\n",
      "labels:\n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "labels.shape:  (6, 1) \n",
      "\n",
      "test_predictions[2]:  [[1], [1], [1], [1], [1], [1]] \n",
      "\n",
      "All data processed.\n",
      "\n",
      "accuracy:  1.0    precision:  1.0    recall:  1.0\n",
      "Test - F1 Score:  1.0 \n",
      "\n",
      "#######\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input_paths = [\"train1_with_2_label_classes.csv\"]\n",
    "\n",
    "minibatch_size = 6\n",
    "num_classes = 2\n",
    "\n",
    "validation(train_input_paths, minibatch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "www = [[0,0,0,1,2,1],\n",
    "       [1,1,1,2,2,0],\n",
    "       [2,2,2,1,1,1]]\n",
    "\n",
    "print(www[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,1,1]\n",
    "b = [2,2,2]\n",
    "\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
